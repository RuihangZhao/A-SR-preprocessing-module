{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainFSRCNNPlusLIghterNet.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMjUdorGnObeRUs6MimpYDS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hdg0HXylaroZ"},"source":["from torch.autograd import Variable\n","import numpy as np\n","import tensorflow as tf\n","import time, math, glob\n","import scipy.io as sio\n","import torch\n","import torch.nn as nn\n","from math import sqrt\n","import argparse, os\n","import torch\n","import random\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.utils.data as data\n","import h5py\n","\n","from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","import copy\n","\n","import numpy as np\n","import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from PIL.ImageOps import colorize\n","\n","import h5py as h5py\n","import numpy as np\n","import tensorflow as tf\n","from sklearn import metrics\n","import torchvision.models as models\n","from PIL import Image\n","import imageio\n","import matplotlib.pyplot as plt\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JCgW9NFla0tk","executionInfo":{"status":"ok","timestamp":1630281961061,"user_tz":-480,"elapsed":24149,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"53a78496-f410-4875-d4b7-67dd3f845f84"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = \"/content/drive/MyDrive\"\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Colab Notebooks',\n"," 'content',\n"," 'drive',\n"," 'train',\n"," 'validation',\n"," 'Cifar10(half-half)_128_lr=0.05_epochs=50.h5',\n"," 'Cifar10(half-half)_128_lr=0.05_epochs=50.h5（副本）',\n"," 'checkpoint',\n"," 'Cifar10_Aug',\n"," 'merged_Cifar10(origin)_128_lr=0.002_epochs=100.h5',\n"," '0',\n"," '5',\n"," 'tensorboard',\n"," 'Cifar10_split',\n"," 'TIP Moire Pattern Removal',\n"," 'data']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHGN19zfbu-Y","executionInfo":{"status":"ok","timestamp":1630282065282,"user_tz":-480,"elapsed":104231,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"3f0942f2-c071-4b2a-f47e-955b6352f1b4"},"source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(128),\n","        transforms.RandomRotation(20),\n","        transforms.ColorJitter(),\n","        #transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(128),\n","        #transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(128),\n","        #transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# data_dir = '/content/drive/MyDrive/Cifar10-VDSR(256)_split'\n","data_dir = '/content/drive/MyDrive/Cifar10_split'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val', 'test']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n","                                             shuffle=True, num_workers=8, drop_last=True)\n","              for x in ['train', 'val', 'test']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NWOAcfTXcdqQ"},"source":["class Net(torch.nn.Module):\n","    def __init__(self, num_channels=3, upscale_factor=8, d=64, s=12, m=4):\n","        super(Net, self).__init__()\n","\n","        self.first_part = nn.Sequential(nn.Conv2d(in_channels=num_channels, out_channels=d, kernel_size=5, stride=1, padding=2),\n","                                        nn.PReLU())\n","\n","        self.layers = []\n","        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=d, out_channels=s, kernel_size=1, stride=1, padding=0),\n","                                         nn.PReLU()))\n","        for _ in range(m):\n","            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1))\n","        self.layers.append(nn.PReLU())\n","        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=s, out_channels=d, kernel_size=1, stride=1, padding=0),\n","                                         nn.PReLU()))\n","\n","        self.mid_part = torch.nn.Sequential(*self.layers)\n","\n","        # Deconvolution\n","        self.last_part = nn.ConvTranspose2d(in_channels=d, out_channels=num_channels, kernel_size=9, stride=upscale_factor, padding=3, output_padding=1)\n","\n","    def forward(self, x):\n","        out = self.first_part(x)\n","        out = self.mid_part(out)\n","        out = self.last_part(out)\n","        return out\n","\n","    def weight_init(self, mean=0.0, std=0.02):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                m.weight.data.normal_(mean, std)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            if isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data.normal_(0.0, 0.0001)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","def conv_layer(chann_in, chann_out, k_size, p_size):\n","    layer = nn.Sequential(\n","        nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n","        nn.BatchNorm2d(chann_out),\n","        nn.ReLU()\n","    )\n","    return layer\n","\n","def vgg_fc_layer(size_in, size_out):\n","    layer = nn.Sequential(\n","        nn.Linear(size_in, size_out),\n","        nn.BatchNorm1d(size_out),\n","        nn.ReLU()\n","    )\n","    return layer\n","\n","def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n","\n","    layers = [conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list))]\n","    layers += [nn.MaxPool2d(kernel_size=pooling_k, stride=pooling_s)]\n","    return nn.Sequential(*layers)\n","\n","class SDCNN(nn.Module):\n","    def __init__(self, n_classes=10):\n","        super(SDCNN, self).__init__()\n","\n","        # Conv blocks (BatchNorm + ReLU activation added in each block)\n","        self.layer1 = vgg_conv_block([3,32], [32,32], [3,3], [1,1], 2, 2)\n","        self.layer2 = vgg_conv_block([32,64], [64,64], [3,3], [1,1], 2, 2)\n","\n","        # FC layers\n","        self.layer3 = vgg_fc_layer(63*63*64, 512)  # 4096->smaller\n","\n","        # Final layer\n","        self.layer4 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        # 从这里开始\n","        out = self.layer1(x)\n","        features = self.layer2(out)\n","        out = features.view(out.size(0), -1)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0cSS70WZ8LU"},"source":["import torch\n","torch.cuda.set_device(0)\n","\n","import os\n","os.environ['CUDA_ENABLE_DEVICES'] = '0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TJDtCp_c6C_s05zgfeE5VTYf_OQT58nG"},"id":"AC6Jv4GMdEwO","executionInfo":{"status":"error","timestamp":1630289366271,"user_tz":-480,"elapsed":6702619,"user":{"displayName":"胡应东","photoUrl":"","userId":"16123890978426666287"}},"outputId":"290d908b-25ae-4c46-fd7e-c5863323ee01"},"source":["# Training settings\n","parser = argparse.ArgumentParser(description=\"PyTorch VDSR\")\n","parser.add_argument(\"--nEpochs\", type=int, default=50, help=\"Number of epochs to train for\")\n","parser.add_argument(\"--lr\", type=float, default=0.1, help=\"Learning Rate. Default=0.1\")\n","parser.add_argument(\"--step\", type=int, default=10, help=\"Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=10\")\n","parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda?\")\n","parser.add_argument(\"--resume\", default=\"\",type=str, help=\"Path to checkpoint (default: none)\")\n","parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")\n","parser.add_argument(\"--clip\", type=float, default=0.4, help=\"Clipping Gradients. Default=0.4\")\n","parser.add_argument(\"--threads\", type=int, default=1, help=\"Number of threads for data loader to use, Default: 1\")\n","parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"Momentum, Default: 0.9\")\n","parser.add_argument(\"--weight-decay\", \"--wd\", default=1e-4, type=float, help=\"Weight decay, Default: 1e-4\")\n","parser.add_argument('--pretrained_SR', default='/content/drive/MyDrive/checkpoint/model_FSRCNN_CNET_epoch_0.pth', type=str, help='path to pretrained model (default: none)')\n","parser.add_argument('--pretrained_TL', default='/content/drive/MyDrive/checkpoint/TransferLearning/model_CNET_epoch_0.pth', type=str, help='path to pretrained model (default: none)')\n","parser.add_argument(\"--gpus\", default=\"0\", type=str, help=\"gpu ids (default: 0)\")\n","parser.add_argument(\"--pretrained_SR_num\", default=0, type=int, help=\"numbers of epochs that have been trained\")\n","parser.add_argument(\"--pretrained_TL_num\", default=0, type=int, help=\"numbers of epochs that have been trained\")\n","parser.add_argument(\"--SRtrain\", default=False, help=\"if train the super resolution network\")\n","parser.add_argument(\"--TLtrain\", default=True, help=\"if train the trainsfer learning network\")\n","parser.add_argument(\"--SR_used\", default=False, help=\"if use the SR method thought the pipeline\")\n","\n","def main():\n","    global opt\n","    opt = parser.parse_args(args=[])\n","    print(opt)\n","\n","    cuda = opt.cuda\n","    if cuda:\n","        print(\"=> use gpu id: '{}'\".format(opt.gpus))\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpus\n","        if not torch.cuda.is_available():\n","                raise Exception(\"No GPU found or Wrong gpu id, please run without --cuda\")\n","\n","    opt.seed = random.randint(1, 10000)\n","    print(\"Random Seed: \", opt.seed)\n","    torch.manual_seed(opt.seed)\n","    if cuda:\n","        torch.cuda.manual_seed(opt.seed)\n","\n","    cudnn.benchmark = True\n","\n","    print(\"===> Loading datasets\")\n","    training_data_loader = dataloaders\n","\n","    print(\"===> Building model\")\n","    SRmodel = Net()\n","    CNET = SDCNN()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(\"===> Setting GPU\")\n","    SRmodel = SRmodel.cuda()\n","    criterion = criterion.cuda()\n","    CNET = CNET.cuda()\n","\n","    # optionally resume from a checkpoint\n","    if opt.resume:\n","        if os.path.isfile(opt.resume):\n","            print(\"=> loading checkpoint '{}'\".format(opt.resume))\n","            checkpoint = torch.load(opt.resume)\n","            opt.start_epoch = checkpoint[\"epoch\"] + 1\n","            SRmodel.load_state_dict(checkpoint[\"model\"].state_dict())\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(opt.resume))\n","\n","    # optionally copy weights from a checkpoint\n","    if opt.pretrained_SR:\n","        if os.path.isfile(opt.pretrained_SR):\n","            print(\"=> loading SR model '{}'\".format(opt.pretrained_SR))\n","            weights = torch.load(opt.pretrained_SR)\n","            SRmodel.load_state_dict(weights['model'].state_dict())\n","        else:\n","            print(\"=> no model found at '{}'\".format(opt.pretrained_SR))  \n","    \n","    if opt.pretrained_TL:\n","        if os.path.isfile(opt.pretrained_TL):\n","            print(\"=> loading TL model '{}'\".format(opt.pretrained_TL))\n","            weights = torch.load(opt.pretrained_TL)\n","            #CNET.load_state_dict(weights)\n","            CNET.load_state_dict(weights['model'].state_dict())\n","        else:\n","            print(\"=> no model found at '{}'\".format(opt.pretrained_TL)) \n","\n","    print(\"===> Setting Optimizer\")\n","    optimizer_SR = optim.SGD(SRmodel.parameters(), lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\n","    #optimizer_TL = optim.SGD(vgg16.fc.parameters(), lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\n","    #optimizer_TL = optim.SGD(vgg16.parameters(), lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\n","    optimizer_TL = optim.SGD(CNET.parameters(), lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\n","\n","\n","    print(\"===> Training\")\n","    for epoch in range(opt.start_epoch, opt.nEpochs + 1):\n","        train(dataloaders, optimizer_SR, optimizer_TL, SRmodel, CNET, criterion, epoch)\n","        if opt.SR_used:\n","            save_SR_checkpoint(SRmodel, epoch)\n","        if opt.TLtrain:\n","            save_TL_checkpoint(CNET, epoch)\n","\n","def adjust_learning_rate(optimizer_SR, optimizer_TL, epoch):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n","    lr = opt.lr * (0.1 ** (epoch // opt.step))\n","    return lr\n","\n","def train(training_data_loader, optimizer_SR, optimizer_TL, SRmodel, CNET, criterion, epoch):\n","    lr = adjust_learning_rate(optimizer_SR, optimizer_TL, epoch-1)\n","\n","    for param_group in optimizer_SR.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","    for param_group in optimizer_TL.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","    print(\"Epoch = {}, lr = {}\".format(epoch, optimizer_SR.param_groups[0][\"lr\"]))\n","\n","    #total = 0  \n","    #count = 0\n","    #false = 0\n","\n","    SRmodel.train()\n","\n","    total = 0\n","\n","    for iteration, batch in enumerate(training_data_loader['train'], 1):\n","\n","      images, labels = Variable(batch[0]), Variable(batch[1], requires_grad=False)\n","      images = images.cuda()\n","      labels = labels.cuda()\n","\n","      if opt.SR_used:\n","        out_images = SRmodel(images)\n","        output = CNET(out_images)\n","      else:\n","        output = CNET(images)\n","      \n","      predicted = output\n","      #total += labels.size(0)\n","      #false += (torch.max(predicted.cpu(),1) != labels).sum()\n","      #count = count+1\n","\n","      loss = criterion(predicted,labels)\n","      if opt.SRtrain:\n","        optimizer_SR.zero_grad()\n","      if opt.TLtrain:\n","        optimizer_TL.zero_grad()\n","      \n","      loss.backward() \n","      nn.utils.clip_grad_norm_(SRmodel.parameters(),opt.clip)\n","      nn.utils.clip_grad_norm_(CNET.parameters(),opt.clip)\n","\n","      if opt.SRtrain:\n","        optimizer_SR.step()\n","      if opt.TLtrain:\n","        optimizer_TL.step()\n","\n","      total += loss.item()\n","\n","      if iteration%100 == 0:\n","          print(\"===> Average Loss: {:.10f}\".format(total/iteration))\n","\n","    if opt.SR_used:\n","        # Get a batch of training data\n","        out_images, classes = out_images.cpu(), labels.cpu()\n","        inputs = images.cpu()\n","\n","        # Make a grid from batch\n","        out_images = torchvision.utils.make_grid(out_images)\n","        inp = torchvision.utils.make_grid(inputs)\n","\n","        imshow(out_images, title=[class_names[x] for x in classes])\n","        imshow(inp, title=[class_names[x] for x in classes])\n","\n","def imshow(inp, title=None):\n","      \"\"\"Imshow for Tensor.\"\"\"\n","      inp = inp.numpy().transpose((1, 2, 0))\n","      mean = np.array([0.485, 0.456, 0.406])\n","      std = np.array([0.229, 0.224, 0.225])\n","      inp = std * inp + mean\n","      inp = np.clip(inp, 0, 1)\n","      plt.imshow(inp)\n","      if title is not None:\n","         plt.title(title)\n","      plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","def save_SR_checkpoint(SRmodel, epoch):\n","    model_out_path = \"/content/drive/MyDrive/checkpoint/\" + \"model_FSRCNN_CNET_*8_epoch_{}.pth\".format(epoch+opt.pretrained_SR_num)\n","    state = {\"epoch\": epoch ,\"model\": SRmodel}\n","    if not os.path.exists(\"/content/drive/MyDrive/checkpoint/\"):\n","        os.makedirs(\"/content/drive/MyDrive/checkpoint/\")\n","\n","    torch.save(state, model_out_path)\n","\n","    print(\"Super resolution network checkpoint saved to {}\".format(model_out_path))\n","\n","def save_TL_checkpoint(model, epoch):\n","    model_out_path = \"/content/drive/MyDrive/checkpoint/TransferLearning/\" + \"model_FSRCNN_CNET_*8_epoch_{}.pth\".format(epoch+opt.pretrained_TL_num)\n","    state = {\"epoch\": epoch ,\"model\": model}\n","    if not os.path.exists(\"/content/drive/MyDrive/checkpoint/TransferLearning/\"):\n","        os.makedirs(\"/content/drive/MyDrive/checkpoint/TransferLearning/\")\n","\n","    torch.save(state, model_out_path)\n","\n","    print(\"Transfer learning checkpoint saved to {}\".format(model_out_path))\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}